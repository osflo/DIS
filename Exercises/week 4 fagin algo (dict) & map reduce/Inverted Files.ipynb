{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IEHaoaicRm-"
   },
   "source": [
    "## ðŸ“š Exercise 2: Inverted Files\n",
    "\n",
    "In this exercise, we will understand the functioning of the Inverted Files and how implement them using MapReduce.\n",
    "\n",
    "### Overview\n",
    "\n",
    "We learnt in the lecture that terms are typically stored in an inverted list. Now, in the inverted list, instead of only storing document identifiers of the documents in which the term appears, assume we also store an *offset* of the appearance of a term in a document. An $offset$ of a term $l_k$ given a document is defined as the number of words between the start of the document and $l_k$. Thus our inverted list is now:\n",
    "\n",
    "$l_k= \\langle f_k: \\{d_{i_1} \\rightarrow [o_1,\\ldots,o_{n_{i_1}}]\\}, \n",
    "\\{d_{i_2} \\rightarrow [o_1,\\ldots,o_{n_{i_2}}]\\}, \\ldots, \n",
    "\\{d_{i_k} \\rightarrow [o_1,\\ldots,o_{n_{i_k}}]\\} \\rangle$\n",
    "\n",
    "This means that in document $d_{i_1}$ term $l_k$ appears $n_{i_1}$ times and at offset $[o_1,\\ldots,o_{n_{i_1}}]$, where $[o_1,\\ldots,o_{n_{i_1}}]$ are sorted in ascending order, these type of indices are also known as term-offset indices. An example of a term-offset index is as follows:\n",
    "\n",
    "**Obama** = $âŸ¨4 : {1 â†’ [3]},{2 â†’ [6]},{3 â†’ [2,17]},{4 â†’ [1]}âŸ©$\n",
    "\n",
    "**Governor** = $âŸ¨2 : {4 â†’ [3]}, {7 â†’ [14]}âŸ©$\n",
    "\n",
    "**Election** = $âŸ¨4 : {1 â†’ [1]},{2 â†’ [1,21]},{3 â†’ [3]},{5 â†’ [16,22,51]}âŸ©$\n",
    "\n",
    "Which is to say that the term **Governor** appear in 2 documents. In document 4 at offset 3, in document 7 at offset 14. Now let us consider the *SLOP/x* operator in text retrieval. This operator has the syntax: *QueryTerm1 SLOP/x QueryTerm2* finds occurrences of *QueryTerm1* within $x$ (but not necessarily in that order) words of *QueryTerm2*, where $x$ is a positive integer argument ($x \\geq 1$). Thus $x = 1$ demands that *QueryTerm1* be adjacent to *QueryTerm2*.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. List the set of documents that satisfy the query **Obama** *SLOP/2* **Election**.\n",
    "2. List each set of values for which the query **Obama** *SLOP/x* **Election** has a different set of documents as answers (starting from $x = 1$). \n",
    "3. Consider the general procedure for \"merging\" two term-offset inverted lists for a given document, to determine where the document satisfies a *SLOP/x* clause (since in general there will be many offsets at which each term occurs in a document). Let $L$ denote the total number of occurrences of the two terms in the document. Assume we have a pointer to the list of occurrences of each term and can move the pointer along this list. As we do so we check whether we have a hit for $SLOP/x$ (i.e. the $SLOP/x$ clause is satisfied). Each move of either pointer counts as a step. Based on this assumption is there a general \"merging\" procedure to determine whether the document satisfies a $SLOP/x$ clause, for which the following is true? Justify your answer.\n",
    "\n",
    "    1. The merge can be accomplished in a number of steps linear in $L$ regardless of $x$, and we can ensure that each pointer moves only to the right (i.e. forward).\n",
    "    2. The merge can be accomplished in a number of steps linear in $L$, but a pointer may be forced to move to the left (i.e. backwards).\n",
    "    3. The merge can require $x \\times L$ steps in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67Z8njyccRm_"
   },
   "source": [
    "## Create inverted files using MapReduce\n",
    "In this exercise, we want to create the inverted files using the MapReduce framework. Write the **map** and **reduce** function.\n",
    "Note that the code of **run_mapreduce** is just a simulation of the behavior of  map-reduce system.\n",
    "\n",
    "## Goal:\n",
    "- Create inverted files using the MapReduce framework.  \n",
    "- Test the implementation on `epfldocs.txt` data.\n",
    "\n",
    "### What you are learning in this exercise:\n",
    "- Having a better understanding of inverted files.\n",
    "- Learn how to implement inverted files using the MapReduce framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVXQdsCMcRnA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fosmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fosmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 135: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\DIS\\Exercises\\week 4\\Inverted Files.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fosmo/OneDrive/Documents/GitHub/DIS/Exercises/week%204/Inverted%20Files.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(word\u001b[39m.\u001b[39mlower()) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fosmo/OneDrive/Documents/GitHub/DIS/Exercises/week%204/Inverted%20Files.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepfldocs.txt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fosmo/OneDrive/Documents/GitHub/DIS/Exercises/week%204/Inverted%20Files.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     content \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mreadlines()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fosmo/OneDrive/Documents/GitHub/DIS/Exercises/week%204/Inverted%20Files.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m original_documents \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m content] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fosmo/OneDrive/Documents/GitHub/DIS/Exercises/week%204/Inverted%20Files.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m documents \u001b[39m=\u001b[39m [tokenize(d)\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m original_documents]\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8f in position 135: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    It tokenizes and stems an input text.\n",
    "    \n",
    "    :param text: str, with the input text\n",
    "    :return: list, of the tokenized and stemmed tokens.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "docs_n_doc_ids = list(zip(range(len(documents)), documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_p9DKAPXcRnD"
   },
   "outputs": [],
   "source": [
    "# Take a pair (doc_id, content). Example: (0, ['how', 'to', 'bake', 'bread', 'without', 'bake', 'recip'])\n",
    "# Return: list of (word, doc_id). Example: ('bake', 0)\n",
    "# For simplicity, we assume each mapper handles a document\n",
    "def map_function(doc_id, content):\n",
    "    keyvals = []\n",
    "    for word in content:\n",
    "        keyvals.append((word,doc_id)) #attention doc id\n",
    "    return keyvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-SAH9PLcRnG"
   },
   "outputs": [],
   "source": [
    "# Take a list of pairs (word, doc_id). Example:  [('bake', 0), ('bake', 0), ('bake', 3)]\n",
    "# Return: (word, frequency, list of document ids). Example ('bake', 3, [0, 0, 3])\n",
    "# For simplicity, we assume each reducer handles a word\n",
    "def reduce_function(lst):\n",
    "    total = 0\n",
    "    word = lst[0][0]\n",
    "    doc_ids = []\n",
    "    for w, doc_id in lst:\n",
    "        assert(word==w) # Assume each reducer handles 1 word\n",
    "        total=total+1\n",
    "        doc_ids.append(doc_id)\n",
    "    return (word, total, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqUN9lIHcRnK"
   },
   "outputs": [],
   "source": [
    "# We simulate the mapreduce framework by this function\n",
    "def run_mapreduce(docs_n_doc_ids):\n",
    "    # Map phase\n",
    "    key_values = []\n",
    "    for doc_id, doc_content in docs_n_doc_ids:\n",
    "        key_values.extend(map_function(doc_id, doc_content))\n",
    "    # Shuffle phase\n",
    "    values = set(map(lambda x:x[0], key_values))\n",
    "    grouped_key_values = [[y for y in key_values if y[0]==x] for x in values]\n",
    "    # Reduce phase\n",
    "    inverted_files = []\n",
    "    for grouped_key_value in grouped_key_values:\n",
    "        word, total, doc_ids = reduce_function(grouped_key_value)\n",
    "        inverted_files.append((word, total, doc_ids))\n",
    "    return inverted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bV3h1-vZcRnO"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_n_doc_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fosmo\\OneDrive\\Documents\\GitHub\\DIS\\Exercises\\week 4\\Inverted Files.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fosmo/OneDrive/Documents/GitHub/DIS/Exercises/week%204/Inverted%20Files.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run_mapreduce(docs_n_doc_ids)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs_n_doc_ids' is not defined"
     ]
    }
   ],
   "source": [
    "run_mapreduce(docs_n_doc_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "45ea560201869e51ea224bd6c49e0a1394de1cf7a135b36d38ece5cec85bc412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
