{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 13: Entity & Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Relation extraction from Wikipedia articles\n",
    "\n",
    "Use Wikipedia to extract the relation `directedBy(Movie, Person)` by applying pattern based heuristics that utilize: *Part Of Speech Tagging*, *Named Entity Recognition* and *Regular Expressions*.\n",
    "\n",
    "#### Required Library: SpaCy\n",
    "- ```conda install -y spacy```\n",
    "- ```python -m spacy download en```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mjson\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mcsv\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     38\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     52\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[0;32m     53\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fosmo\\anaconda3\\lib\\site-packages\\spacy\\util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import urllib.request, json, csv, re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tsv with input movies\n",
    "def read_tsv():\n",
    "    movies=[]\n",
    "    with open('movies.tsv','r') as file:\n",
    "        tsv = csv.reader(file, delimiter='\\t')\n",
    "        next(tsv) #remove header\n",
    "        movies = [{'movie':line[0], 'director':line[1]} for line in tsv]\n",
    "    return movies\n",
    "\n",
    "#parse wikipedia page\n",
    "def parse_wikipedia(movie):\n",
    "    txt = ''\n",
    "    try:\n",
    "        with urllib.request.urlopen('https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=&explaintext=&titles='+movie) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            txt = next (iter (data['query']['pages'].values()))['extract']\n",
    "    except:\n",
    "        pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Parse the raw text of a Wikipedia movie page and extract named (PER) entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_PER_entities(txt):\n",
    "    persons = []\n",
    "    for t in txt.ents:\n",
    "        if t.label=='PERSON':\n",
    "            persons.append(t)\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Given the raw text of a Wikipedia movie page and the extracted PER entities, find the director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_director(txt, persons):\n",
    "    txt = re.sub('[!?,.]', '', txt).split()\n",
    "        for t in txt.ents:\n",
    "            if\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = read_tsv()\n",
    "movies[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements=[]\n",
    "for m in movies:\n",
    "\n",
    "        txt = parse_wikipedia(m['movie'])\n",
    "        persons = find_PER_entities(txt)\n",
    "        director = find_director(txt, persons)\n",
    "        \n",
    "        if director != '':\n",
    "            statements.append(m['movie'] + ' is directed by ' + director + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Compute the precision and recall based on the given ground truth (column Director from tsv file) and show examples of statements that are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute precision and recall\n",
    "precision = ...\n",
    "recall = ...\n",
    "print ('Precision:',precision)\n",
    "print ('Recall:',recall)\n",
    "print('\\n***Sample Statements***')\n",
    "print(statements[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition using Hidden Markov Model\n",
    "\n",
    "\n",
    "Define a Hidden Markov Model (HMM) that recognizes Person (*PER*) entities.\n",
    "Particularly, your model must be able to recognize pairs of the form (*firstname lastname*) as *PER* entities.\n",
    "Using the given sentences as training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=['The best blues singer was Bobby Bland while Ray Charles pioneered soul music .', \\\n",
    "              'Bobby Bland was just a singer whereas Ray Charles was a pianist , songwriter and singer .' \\\n",
    "              'None of them lived in Chicago .']\n",
    "\n",
    "test_set=['Ray Charles was born in 1930 .', \\\n",
    "          'Bobby Bland was born the same year as Ray Charles .', \\\n",
    "          'Muddy Waters is the father of Chicago Blues .']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Annotate your training set with the labels I (for PER entities) and O (for non PER entities).\n",
    "\t\n",
    "    *Hint*: Represent the sentences as sequences of bigrams, and label each bigram.\n",
    "\tOnly bigrams that contain pairs of the form (*firstname lastname*) are considered as *PER* entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation\n",
      " [['The best', 'O'], ['best blues', 'O'], ['blues singer', 'O'], ['singer was', 'O'], ['was Bobby', 'O'], ['Bobby Bland', 'I'], ['Bland while', 'O'], ['while Ray', 'O'], ['Ray Charles', 'I'], ['Charles pioneered', 'O'], ['pioneered soul', 'O'], ['soul music', 'O'], ['music .', 'O'], ['Bobby Bland', 'I'], ['Bland was', 'O'], ['was just', 'O'], ['just a', 'O'], ['a singer', 'O'], ['singer whereas', 'O'], ['whereas Ray', 'O'], ['Ray Charles', 'I'], ['Charles was', 'O'], ['was a', 'O'], ['a pianist', 'O'], ['pianist ,', 'O'], [', songwriter', 'O'], ['songwriter and', 'O'], ['and singer', 'O'], ['singer .None', 'O'], ['.None of', 'O'], ['of them', 'O'], ['them lived', 'O'], ['lived in', 'O'], ['in Chicago', 'O'], ['Chicago .', 'O']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bigram Representation\n",
    "def getBigrams(sents):\n",
    "    return [b[0]+' '+b[1] for l in sents for b in zip(l.split(' ')[:-1], l.split(' ')[1:])]\n",
    "\n",
    "bigrams = getBigrams(training_set)\n",
    "\n",
    "#Annotation\n",
    "PER = ['Bobby Bland', 'Ray Charles']\n",
    "annotations = []\n",
    "for b in bigrams:\n",
    "    pers=False\n",
    "    for p in PER:\n",
    "        if b==p:\n",
    "            annotations.append([b,'I'])\n",
    "            pers=True\n",
    "    if not pers:\n",
    "        annotations.append([b,'O'])\n",
    "    \n",
    "print('Annotation\\n', annotations,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Compute the transition and emission probabilities for the HMM (use smoothing parameter $\\lambda$=0.5).\n",
    "\n",
    "    *Hint*: For the emission probabilities you can utilize the morphology of the words that constitute a bigram (e.g., you can count their uppercase first characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "#Transition Probabilities\n",
    "transition_prob={}\n",
    "\n",
    "I_count=0\n",
    "total=len(annotations)\n",
    "for a in annotations:\n",
    "    if a[1]=='I':\n",
    "        I_count+=1\n",
    "\n",
    "#Prior\n",
    "transition_prob['P(I|start)'] = I_count/total\n",
    "transition_prob['P(O|start)'] = 1-I_count/total\n",
    "\n",
    "count_OO=0\n",
    "count_OI=0\n",
    "count_IO=0\n",
    "count_II=0\n",
    "for i,a in enumerate(annotations):\n",
    "    if annotations[i-1]=='O' and a==\n",
    "\n",
    "transition_prob['P(O|O)'] = ...\n",
    "transition_prob['P(O|I)'] = ...\n",
    "transition_prob['P(I|O)'] = ...\n",
    "transition_prob['P(I|I)'] = ...\n",
    "\n",
    "print('Transition Probabilities\\n',transition_prob, '\\n')\n",
    "\n",
    "#Emission Probabilities\n",
    "emission_prob={}\n",
    "\n",
    "        \n",
    "default_emission = ...\n",
    "\n",
    "emission_prob['P(2_upper|O)'] = ...\n",
    "emission_prob['P(2_upper|I)'] = ...\n",
    "emission_prob['P(1_upper|O)'] = ...\n",
    "emission_prob['P(1_upper|I)'] = ...\n",
    "emission_prob['P(0_upper|O)'] = ...\n",
    "emission_prob['P(0_upper|I)'] = ...\n",
    "\n",
    "print('Emission Probabilities\\n', emission_prob, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Predict the labels of the test set and compute the precision and the recall of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "bigrams = getBigrams(test_set)\n",
    "entities=[]\n",
    "prev_state='start'\n",
    "I_prob=\n",
    "for b in bigrams:\n",
    "    I_prob = I_prob*\n",
    "    O_prob = ...\n",
    "    \n",
    "    if ...:\n",
    "        prev_state = 'O'\n",
    "    else:\n",
    "        entities.append(b)\n",
    "        prev_state = 'I'\n",
    "\n",
    "print('Predicted Entities\\n', entities, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is *...%* while recall is *...%*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comment on how you can further improve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "45ea560201869e51ea224bd6c49e0a1394de1cf7a135b36d38ece5cec85bc412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
