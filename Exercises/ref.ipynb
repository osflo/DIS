{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import math\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('french')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.zeros((n_nodes, n_nodes))\n",
    "np.linalg.norm(P,2) #for sum take 1\n",
    "np.linalg.eig(P)\n",
    "#Take max eigen value:\n",
    "np.absolute(eigenvectors[:,np.argmax(np.absolute(eigenvalues))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ca-GrQc.txt\") as f: # don't forget\n",
    "\n",
    "#For file with nodes nodes (s7)\n",
    "    n_nodes = 0\n",
    "    nodes_idx = dict() \n",
    "    nodes = []\n",
    "    for line in f:\n",
    "            if '#' not in line: #don't forget remove useless lines\n",
    "                source = int(line.split()[0])\n",
    "                target = int(line.split()[1])\n",
    "                if source not in nodes_idx.keys():\n",
    "                    nodes_idx[source] = n_nodes\n",
    "                    nodes.append(source)\n",
    "                    n_nodes += 1\n",
    "                if target not in nodes_idx.keys():\n",
    "                    nodes_idx[target] = n_nodes\n",
    "                    nodes.append(target)\n",
    "                    n_nodes += 1\n",
    "\n",
    "#cvs :\n",
    "df = pd.read_csv('data/ml-100k/u.data', sep='\\t', names=header)\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "df.describe()\n",
    "#data from df : how to go line by line\n",
    "for line in data.itertuples():\n",
    "        data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "\n",
    "#info on cvs\n",
    "articles = pd.read_csv('data/news_articles.txt', sep='|').fillna('')\n",
    "log = pd.read_csv('data/user_log.txt', sep='|')\n",
    "articles.head()\n",
    "articles['corpus'].str.split(expand=True).stack().value_counts().rename_axis('word').reset_index(name='counts')[:20]#top sorted\n",
    "articles['medium'].value_counts().rename_axis('medium').reset_index(name='counts')#all sorted\n",
    "articles['authors'].apply(lambda x: 1 if len(x.split(','))==1 else 0).sum()/ articles['authors'].count() #% for one author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare corpus and tf idf\n",
    "stemmer = PorterStemmer() \n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    It tokenizes and stems an input text.\n",
    "    \n",
    "    :param text: str, with the input text\n",
    "    :return: list, of the tokenized and stemmed tokens.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d) for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()\n",
    "#or\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top k , p proba vector\n",
    "arr = np.array(p[:,0])\n",
    "k = 3\n",
    "k_idx = arr.argsort()[-k:][:3] # index of top-k nodes\n",
    "#or\n",
    "ans = sorted(tfidf.items(), key=lambda x:x[1], reverse = True)[:k]\n",
    "#or\n",
    "scores.sort(key=lambda x: -x[0])\n",
    "\n",
    "#different\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "#non zeros\n",
    ".nonzero()\n",
    ".flatten()\n",
    "\n",
    "#sets, retirer un el :\n",
    "set1-el"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TextClassificationDataset.build_vocab(tokenizer, split='train', min_freq=2)\n",
    "model = AttentionModel(train_set.vocab_size(),train_set.num_classes())\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "train_dataloader = DataLoader(train_set,batch_size=128, shuffle=True, collate_fn=model.collate_batch)\n",
    "\n",
    "test_set = TextClassificationDataset(tokenizer, text_vocab=train_set.text_vocab, label_vocab=train_set.label_vocab,\n",
    "    split='test')\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=128*2, shuffle=False, collate_fn=model.collate_batch)\n",
    "#Attention le vocab est defini par le train set, si test avec Knn utiliser le vocab du train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity and scores computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "item_similarity = 1-pairwise_distances(train_data_matrix.T, metric='cosine') \n",
    "#train data matrix has line user col item, here similarity btw items\n",
    "\n",
    "def compute_recall_at_k(predict, gt, k):\n",
    "    correct_recall = set(predict[:k]).intersection(set(gt))\n",
    "    return len(correct_recall)/len(gt)\n",
    "\n",
    "def compute_precision_at_k(predict, gt, k):\n",
    "    correct_predict = set(predict[:k]).intersection(set(gt))\n",
    "    return len(correct_predict)/k\n",
    "\n",
    "def f1score(predict, gt, k):\n",
    "    prec = compute_precision_at_k(predict, gt, k)\n",
    "    rec = compute_recall_at_k(predict, gt, k)\n",
    "    print(prec, rec)\n",
    "    return 2*prec*rec/(prec+rec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity matrix and svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_term_document_matrix(vocabulary, documents):\n",
    "    # Construct term-doc matrix\n",
    "    matrix = np.zeros((len(vocabulary), len(documents)))\n",
    "    for j, document in enumerate(documents):\n",
    "        counter = Counter(document.split())\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            if word in counter:\n",
    "                matrix[i,j] = counter[word]\n",
    "    return matrix\n",
    "\n",
    "def truncated_svd(term_doc_matrix, num_val):\n",
    "    K, S, Dt = np.linalg.svd(term_doc_matrix, full_matrices=False)\n",
    "    K_sel = K[:,0:num_val]\n",
    "    S_sel = np.diag(S)[0:num_val,0:num_val]\n",
    "    Dt_sel = Dt[0:num_val,:]\n",
    "    return K_sel, S_sel, Dt_sel\n",
    "\n",
    "def construct_query_vector(query, vocabulary, K_sel, S_sel):\n",
    "    q = query_to_document_vector(query, vocabulary)\n",
    "    mapper = np.dot(K_sel, np.linalg.inv(S_sel))\n",
    "    q_trans =  np.dot( q, mapper)\n",
    "    return q_trans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45ea560201869e51ea224bd6c49e0a1394de1cf7a135b36d38ece5cec85bc412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
